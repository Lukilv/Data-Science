{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuSxPbblHJK1"
      },
      "outputs": [],
      "source": [
        "# задача 1\n",
        "\n",
        "# Найти корни квадратного уравнения методом градиентного спуска\n",
        "# x ** 2 - 5 * x + 4 = 0\n",
        "# надо начать движение от начальной точки в направлении антиградиента с заданным шагом\n",
        "# x = x - learning_rate * grad(x)\n",
        "# всегда ли сойдемся за приемлемое количество шагов?\n",
        "# важна ли начальная точка?\n",
        "# как найти второй корень?\n",
        "# как влияет ЛР?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6iTQQ4bO_64l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, b, c):\n",
        "    #Функция квадратного уравнения\n",
        "    return x**2 - b*x + c\n",
        "\n",
        "def df(x, b, c):\n",
        "    #Производная от квадрата квадратного уравнения\n",
        "    return 2*(x**2 - b*x + c)*(2*x - b)\n",
        "\n",
        "def gradient_descent(b, c, initial_x, learning_rate, epochs):\n",
        "   #Градиентный спуск\n",
        "    x = initial_x\n",
        "    for i in range(epochs):\n",
        "        grad = df(x, b, c)\n",
        "        x = x - learning_rate * grad\n",
        "        if abs(f(x, b, c)) < 1e-6:  #Проверка насколько близко ошибка подошла к 0\n",
        "            break\n",
        "    return x\n",
        "\n",
        "b, c = 5, 4\n",
        "\n",
        "# Параметры градиента\n",
        "initial_x = 0  # начальная точка\n",
        "learning_rate = 0.01  # Learning rate\n",
        "epochs = 1000  # Количество итераций\n",
        "\n",
        "# первый корень\n",
        "root = gradient_descent(b, c, initial_x, learning_rate, epochs)\n",
        "print(root)\n",
        "\n",
        "# второй корень с другой начальной точкой\n",
        "initial_x = 3\n",
        "root2 = gradient_descent(b, c, initial_x, learning_rate, epochs)\n",
        "print(root2)\n",
        "\n",
        "\n",
        "# Второй корень можно найти, используя другую начальную точку\n",
        "# Начальная точка влияет сильно на результат\n",
        "# ЛР влияет на скорость обучения, если будет слишком большой, то можно проскочить минимум ошибки, если будет слишком низкий, то можно застрять в локальном минимуме.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjU0n5R21xdj",
        "outputId": "f0ad360a-67c1-4552-e95f-fc50f36e0593"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9999996969264429\n",
            "3.999999672449243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPjNVr3xP1Xl"
      },
      "outputs": [],
      "source": [
        "# Задача 2\n",
        "\n",
        "# Реализовать адаптивный оптимизатор с подстраивающимся LR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdagradOptimizer:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.eps = 1e-8  #\n",
        "        self.cache = None\n",
        "\n",
        "    def step(self, params, grads):\n",
        "        if self.cache is None:\n",
        "            self.cache = [np.zeros_like(param) for param in params]\n",
        "\n",
        "        for i, param in enumerate(params):\n",
        "            self.cache[i] += grads[i] ** 2\n",
        "            adjusted_grad = grads[i] / (np.sqrt(self.cache[i]) + self.eps)\n",
        "            params[i] -= self.learning_rate * adjusted_grad  #\n"
      ],
      "metadata": {
        "id": "LvCmkYxRAszj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdagradOptimizer(learning_rate=0.1)\n",
        "params = [np.random.randn(10, 10), np.random.randn(10)]\n",
        "\n",
        "grads = [np.random.randn(*param.shape) for param in params]\n",
        "\n",
        "optimizer.step(params, grads)\n"
      ],
      "metadata": {
        "id": "L1guVAjPDSGU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "l-iXHAA1Fz1p"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "a2swiHK-HIOq"
      },
      "outputs": [],
      "source": [
        "# Task 2\n",
        "# Realize forward and backward pass for linear layer with sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ibFFthYnHIlh"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1. / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_backward(da, x):\n",
        "    sig = sigmoid(x)\n",
        "\n",
        "    return da * sig * (1 - sig)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0., x)\n",
        "\n",
        "def relu_backward(da, x):\n",
        "    da = np.array(da, copy = True)\n",
        "    da[x <= 0] = 0\n",
        "    return da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gvMKpB5WFz1z"
      },
      "outputs": [],
      "source": [
        "def mse_loss(t, y):\n",
        "    return (t - y) ** 2\n",
        "\n",
        "def d_mse_loss(t, y):\n",
        "    return 2 * (y - t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-qAVeaVcFz10"
      },
      "outputs": [],
      "source": [
        "class LinearLayer:\n",
        "    def __init__(self, n_inp, n_out, activation='sigmoid'):\n",
        "        self.w = np.random.randn(n_out, n_inp) * 0.1\n",
        "        self.b = np.random.randn(n_out, 1) * 0.1\n",
        "        if activation == 'sigmoid':\n",
        "            self.activ = sigmoid\n",
        "        if activation == 'relu':\n",
        "            self.activ = relu\n",
        "        elif activation == 'None':\n",
        "            self.activ = None\n",
        "        else:\n",
        "            raise Exception(f'Unknown activation \"{activation}\"')\n",
        "        self._clear_state()\n",
        "\n",
        "    def _clear_state(self):\n",
        "        self.lin = None\n",
        "        self.inp = None\n",
        "        self.d_w = None\n",
        "        self.d_b = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.inp = x\n",
        "        self.lin = np.dot(self.w, x) + self.b\n",
        "        activ = self.activ(self.lin) if self.activ is not None else self.lin\n",
        "\n",
        "        return activ\n",
        "\n",
        "    def backward(self, grad): # grad = d L / d z    Dout\n",
        "        # grad * dz / d lin\n",
        "        if self.activ == sigmoid:\n",
        "            grad_lin = sigmoid_backward(grad, self.lin)\n",
        "        if self.activ == relu:\n",
        "            grad_lin = relu_backward(grad, self.lin)\n",
        "        else:\n",
        "            grad_lin = grad\n",
        "        # grad_lin * d lin / d w\n",
        "        m = self.inp.shape[1]\n",
        "        self.d_w = grad_lin @ self.inp.T / m\n",
        "        # grad_lin * d lin / d b\n",
        "        self.d_b = np.sum(grad_lin, axis=1, keepdims=True) / m\n",
        "\n",
        "        grad = np.dot(self.w.T, grad_lin)\n",
        "\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jUZcVU8z2T-t"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, arch: Tuple[Tuple[int, int]], activation):\n",
        "        self.layers = []\n",
        "        for i, p in enumerate(arch):\n",
        "            self.layers.append(\n",
        "                LinearLayer(p[0], p[1],\n",
        "                            activation=activation if i < len(arch)-1 else 'None')\n",
        "                )\n",
        "        self._clear_state()\n",
        "\n",
        "    def _clear_state(self):\n",
        "        for l in self.layers:\n",
        "            l._clear_state()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for l in self.layers:\n",
        "            x = l.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad):\n",
        "        for l in reversed(self.layers):\n",
        "            grad = l.backward(grad)\n",
        "\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmLRIBk4Fz12"
      },
      "outputs": [],
      "source": [
        "# Task 3\n",
        "# Realize SGD Momentum optimizer\n",
        "# velocity = momentum * velocity - lr * gradient\n",
        "# w = w + velocity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "YRCDRdKNn8qs"
      },
      "outputs": [],
      "source": [
        "#для всей модели\n",
        "class SGDMomentum:\n",
        "    def __init__(self, model: Model, lr= 0.0001, momentum=0.9):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.m = momentum\n",
        "        self.vel = [[np.zeros_like(layer.w),\n",
        "                     np.zeros_like(layer.b)] for layer in model.layers]\n",
        "\n",
        "    def step(self):\n",
        "        for i, layer in enumerate(self.model.layers):\n",
        "            self.vel[i][0] = self.vel[i][0] * self.m - self.lr * layer.d_w\n",
        "            self.vel[i][1] = self.vel[i][1] * self.m - self.lr * layer.d_b\n",
        "            layer.w += self.vel[i][0]\n",
        "            layer.b += self.vel[i][1]\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.model._clear_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZPKwvE-Fz15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXoxTHw5Fz16"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AN-E_lK_Fz18"
      },
      "outputs": [],
      "source": [
        "x = np.random.uniform(-3, 3, 20000)\n",
        "y = x**2 + np.random.randn()*0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2aBvwzyFz18",
        "outputId": "e009e1bb-fce7-46f6-b221-6e45073ff7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [[-0.24843854]] [[-0.27199549]] [[-0.26851293]] [[-0.31483425]]\n",
            "1 [[1.64026474]] [[4.52778064]] [[1.65116296]] [[4.46503956]]\n",
            "2 [[1.24338431]] [[4.52817876]] [[1.24530243]] [[4.47272861]]\n",
            "3 [[1.05992496]] [[4.49491844]] [[1.04920875]] [[4.44726259]]\n",
            "4 [[1.0092588]] [[4.45030793]] [[0.99818621]] [[4.41002083]]\n",
            "5 [[0.97891121]] [[4.40243613]] [[0.97327746]] [[4.3695646]]\n",
            "6 [[0.99211186]] [[4.35589742]] [[0.97500115]] [[4.3294755]]\n",
            "7 [[1.00285023]] [[4.31307741]] [[0.98833596]] [[4.29140304]]\n",
            "8 [[1.00830709]] [[4.2739728]] [[0.99470663]] [[4.25596491]]\n",
            "9 [[1.01063024]] [[4.23803633]] [[0.99672082]] [[4.22354877]]\n",
            "10 [[1.01076731]] [[4.20514596]] [[0.99760031]] [[4.19387706]]\n",
            "11 [[1.00933609]] [[4.17509889]] [[0.99721281]] [[4.16658558]]\n",
            "12 [[1.00680766]] [[4.14756277]] [[0.99538112]] [[4.14126407]]\n",
            "13 [[1.00597036]] [[4.12245766]] [[0.99251821]] [[4.11794587]]\n",
            "14 [[1.00841661]] [[4.09944817]] [[0.98900346]] [[4.09603175]]\n",
            "15 [[1.01048633]] [[4.07835959]] [[0.9851544]] [[4.07549491]]\n",
            "16 [[1.0121071]] [[4.05900428]] [[0.98119735]] [[4.05646398]]\n",
            "17 [[1.0134182]] [[4.04112357]] [[0.97722053]] [[4.03890616]]\n",
            "18 [[1.01446906]] [[4.02653746]] [[0.97346037]] [[4.02264161]]\n",
            "19 [[1.01532163]] [[4.02012494]] [[0.96986624]] [[4.00960452]]\n"
          ]
        }
      ],
      "source": [
        "model = Model(((1, 100), (100, 1)), activation='relu')\n",
        "optim = SGDMomentum(model, lr=0.00001)\n",
        "for e in range(20):\n",
        "    print(e, model.forward([[1]]), model.forward([[2]]), model.forward([[-1]]), model.forward([[-2]]))\n",
        "    for i, (val, t) in enumerate(zip(x, y)):\n",
        "        optim.zero_grad()\n",
        "        pred = model.forward(np.array([[val]]))\n",
        "        loss = mse_loss(t, pred)\n",
        "        grad = d_mse_loss(t, pred)\n",
        "        model.backward(grad)\n",
        "        optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_5jAO8pVFz19",
        "outputId": "52921acf-15bf-4c7b-ce9c-f20d2ef7b55e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19 [[1.01600192]] [[4.01714374]] [[0.96643764]] [[483.9757612]]\n"
          ]
        }
      ],
      "source": [
        "print(e, model.forward([[1]]), model.forward([[2]]), model.forward([[-1]]), model.forward([[103]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOOY8douFz1-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zROVnJAcFz1-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QWQyu5NFz1-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}